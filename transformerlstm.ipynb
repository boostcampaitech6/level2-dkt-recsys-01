{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "import gc \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTset():\n",
    "    def __init__(self, DATA_PATH):\n",
    "        self.preprocessing(DATA_PATH)\n",
    "        self.oof_ID_set = self.split_data()\n",
    "    \n",
    "    def split_data(self):\n",
    "        user_list = self.df['userID'].unique().tolist()\n",
    "        oof_ID_set = {}\n",
    "        kfold = KFold(n_splits = 5, shuffle = True, random_state = 4444)\n",
    "        for i, (t, v) in enumerate(kfold.split(user_list)):\n",
    "            oof_ID_set[i] = v.tolist()\n",
    "        \n",
    "        return oof_ID_set\n",
    "    \n",
    "    def preprocessing(self, DATA_PATH):\n",
    "\n",
    "        dtype = {\n",
    "            'userID': 'int16',\n",
    "            'answerCode': 'int8',\n",
    "            'KnowledgeTag': 'int16',\n",
    "\n",
    "        }\n",
    "        \n",
    "        train_df = pd.read_csv(os.path.join(DATA_PATH, 'train_data.csv'), dtype=dtype, parse_dates=['Timestamp'])\n",
    "        # train_df = train_df.drop('Unnamed: 0', axis=1)\n",
    "        test_df = pd.read_csv(os.path.join(DATA_PATH, 'test_data.csv'), dtype=dtype, parse_dates=['Timestamp'])\n",
    "        # test_df = test_df.drop('Unnamed: 0', axis=1)\n",
    "        # test_df = test_df.rename({'assessment_level' : 'beta'},axis=1)\n",
    "        # test_df = test_df.rename({'student_Level' : 'student_level'}, axis=1)\n",
    "\n",
    "        def FE(df: pd.DataFrame) -> pd.DataFrame:\n",
    "            def chg_idx(lst):\n",
    "                tmp = {}\n",
    "                for i,j in enumerate(lst):\n",
    "                    tmp[j] = i\n",
    "                return tmp\n",
    "\n",
    "            def convert_time(s: str):\n",
    "                timestamp = time.mktime(s.timetuple())\n",
    "                return int(timestamp)\n",
    "            df[\"convert_time\"] = df[\"Timestamp\"].apply(convert_time)\n",
    "            df['level'] = df['testId'].apply(lambda x: x[2])\n",
    "            df['shift'] = df['convert_time'].shift(-1).fillna(0).astype(int)\n",
    "            df['elapsed'] = df['shift'] - df['convert_time']\n",
    "            df['check'] = df['userID'].shift(-1)\n",
    "            df['d_check'] = df['testId'].shift(-1)\n",
    "            df.loc[(df['userID'] != df['check']) | (df['testId'] != df['d_check']) | (df['elapsed'] >= 86400), 'elapsed'] = 0\n",
    "\n",
    "            # 정답과 오답의 평균,중간 소요시간\n",
    "            # collect_elp_mean = df[df['answerCode'] == 1].groupby('assessmentItemID')['elapsed'].mean()\n",
    "            # df = df.join(collect_elp_mean, on='assessmentItemID',rsuffix='_1_avg')\n",
    "            # wrong_elp_mean = df[df['answerCode']== 0].groupby('assessmentItemID')['elapsed'].mean()\n",
    "            # df = df.join(wrong_elp_mean, on='assessmentItemID',rsuffix='_0_avg')\n",
    "    \n",
    "            # collect_elp_median = df[df['answerCode'] == 1].groupby('assessmentItemID')['elapsed'].median()\n",
    "            # df = df.join(collect_elp_median, on='assessmentItemID',rsuffix='_1_mdn')\n",
    "            # wrong_elp_median = df[df['answerCode']== 0].groupby('assessmentItemID')['elapsed'].median()\n",
    "            # df = df.join(wrong_elp_median, on='assessmentItemID',rsuffix='_0_mdn')\n",
    "\n",
    "            # 전체 표준편차\n",
    "            elapsed_std = df.groupby('assessmentItemID')['elapsed'].std()\n",
    "            df = df.join(elapsed_std, on='assessmentItemID', rsuffix='_std')\n",
    " \n",
    "            # # 맞춘 인원의 표준편차\n",
    "            # elapsed_1_std = df[df['answerCode'] == 1].groupby('assessmentItemID')['elapsed'].std()\n",
    "            # df = df.join(elapsed_1_std, on='assessmentItemID', rsuffix='_1_std')\n",
    "            # df = df.reset_index(drop=False)\n",
    "\n",
    "            df = df.drop(['shift','check','d_check','convert_time'], axis=1)\n",
    "            # df['Timestamp'] = df['Timestamp'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
    "            df['dayofweek'] = df['Timestamp'].dt.dayofweek\n",
    "            answer_rate = df.groupby('assessmentItemID')['answerCode'].mean()\n",
    "            df = df.join(answer_rate, on='assessmentItemID', rsuffix='_rate')\n",
    "\n",
    "            col_ = {}\n",
    "\n",
    "            assItemID_ = chg_idx(df['assessmentItemID'].unique().tolist())\n",
    "            df['assessmentItemID_'] = df['assessmentItemID'].apply(lambda x: assItemID_[x])\n",
    "            col_['assItemID_'] = assItemID_\n",
    "\n",
    "            testID_ = chg_idx(df['testId'].unique().tolist())\n",
    "            df['testId_'] = df['testId'].apply(lambda x: testID_[x])\n",
    "            col_['testID_'] = testID_\n",
    "            \n",
    "            tag_ = chg_idx(df['KnowledgeTag'].unique().tolist())\n",
    "            df['KnowledgeTag_'] = df['KnowledgeTag'].apply(lambda x: tag_[x])\n",
    "            col_['tag_'] = tag_\n",
    "\n",
    "            level_ = chg_idx(df['level'].unique().tolist())\n",
    "            df['level_'] = df['level'].apply(lambda x: level_[x])\n",
    "            col_['level_'] = level_\n",
    "            \n",
    "            # student_level_ = chg_idx(df['student_level'].unique().tolist())\n",
    "            # df['student_level_'] = df['student_level'].apply(lambda x: student_level_[x])\n",
    "            # col_['student_level_'] = student_level_\n",
    "\n",
    "            # beta_ = chg_idx(df['beta'].unique().tolist())\n",
    "            # df['beta_'] = df['beta'].apply(lambda x: beta_[x])\n",
    "            # col_['beta_'] = beta_\n",
    "\n",
    "\n",
    "            #문제를 푼 인원 \n",
    "            # usr_cnt = df.groupby('assessmentItemID')['userID'].count()\n",
    "            # df = df.join(usr_cnt, on='assessmentItemID', rsuffix='_cnt')\n",
    "            # cnt_ = chg_idx(df['assessmentItemID_cnt'].unique().tolist())\n",
    "            # df['assessmentItemID_cnt_'] = df['assessmentItemID_cnt'].apply(lambda x: cnt_[x])\n",
    "            # col_['cnt_'] = cnt_\n",
    "\n",
    "            return df, col_\n",
    "\n",
    "\n",
    "        train_df, col_ = FE(train_df)\n",
    "        test_df, _  = FE(test_df)\n",
    "\n",
    "        self.assItemID_ = col_['assItemID_']\n",
    "        \n",
    "        self.n_assItemID_ = len(col_['assItemID_'])\n",
    "        self.n_testId_ = len(col_['testID_'])\n",
    "        self.n_tag_ = len(col_['tag_'])\n",
    "        self.n_level_ = len(col_['level_'])\n",
    "        # self.n_cnt_ = len(col_['cnt_'])\n",
    "        # self.n_student_level_ = len(col_['student_level_'])\n",
    "        # self.n_beta_ = len(col_['beta_'])\n",
    "\n",
    "        self.n_dayweek = 7\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.df = pd.concat([train_df, test_df[test_df['answerCode'] != -1]]).reset_index(drop=True)\n",
    "        self.cat_cols = ['assessmentItemID_','testId_','KnowledgeTag_','level_','dayofweek']#'student_level_', 'beta_'\n",
    "        self.con_cols = ['elapsed','answerCode_rate','elapsed_std']\n",
    "        \n",
    "    def get_oof(self, oof):\n",
    "        oof_ID_set_v = self.oof_ID_set[oof]\n",
    "\n",
    "        train = []\n",
    "        valid = []\n",
    "\n",
    "        grob = self.df.groupby('userID')\n",
    "        for usr, df in grob:\n",
    "            if usr in oof_ID_set_v:\n",
    "                train.append(df.iloc[:-1,:])\n",
    "                valid.append(df.copy())\n",
    "            else:\n",
    "                train.append(df)\n",
    "\n",
    "        train = pd.concat(train).reset_index(drop = True)\n",
    "        valid = pd.concat(valid).reset_index(drop = True)\n",
    "\n",
    "        return train, valid\n",
    "    \n",
    "    def get_test_data(self):\n",
    "        return self.test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom(DTset):\n",
    "    def __init__(self,\n",
    "                 df,\n",
    "                 cat_cols = ['assessmentItemID_','testId_','KnowledgeTag_','level_','dayofweek'],#'student_level_', 'beta_'\n",
    "                 con_cols = ['elapsed', 'answerCode_rate','elapsed_std']):\n",
    "        \n",
    "        self.cat_cols = cat_cols\n",
    "        self.con_cols = con_cols\n",
    "        self.get_df = df.groupby('userID')\n",
    "        self.user_lst = df['userID'].unique().tolist()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.user_lst)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        user = self.user_lst[idx]\n",
    "        get_df = self.get_df.get_group(user)\n",
    "\n",
    "        prsnt_df = get_df.iloc[1:,:]\n",
    "        prsnt_cat = prsnt_df[self.cat_cols].values\n",
    "        prsnt_con = prsnt_df[self.con_cols].values\n",
    "        prsnt_answerCode = prsnt_df['answerCode'].values\n",
    "\n",
    "        past_df = get_df.iloc[:-1,:]\n",
    "        past_cat = past_df[self.cat_cols].values\n",
    "        past_con = past_df[self.con_cols].values\n",
    "        past_answerCode = past_df['answerCode'].values\n",
    "\n",
    "        return {'past_cat' : past_cat,\n",
    "                'past_con' : past_con,\n",
    "                'past_answerCode' : past_answerCode,\n",
    "                'prsnt_cat' : prsnt_cat,\n",
    "                'prsnt_con' : prsnt_con,\n",
    "                'prsnt_answerCode' : prsnt_answerCode}\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pad_sequence(seq, max_len, padding_value = 0):\n",
    "    try:\n",
    "        seq_len, col = seq.shape\n",
    "        padding = np.zeros((max_len - seq_len, col)) + padding_value\n",
    "    except:\n",
    "        seq_len = seq.shape[0]\n",
    "        padding = np.zeros((max_len - seq_len, )) + padding_value\n",
    "\n",
    "    padding_seq = np.concatenate([padding, seq])\n",
    "\n",
    "    return padding_seq\n",
    "\n",
    "def train_make_batch(samples):\n",
    "    max_len = 0\n",
    "    for sample in samples:\n",
    "        seq_len, _ = sample['past_cat'].shape\n",
    "        if max_len < seq_len:\n",
    "            max_len = seq_len\n",
    "    \n",
    "    past_cat = []\n",
    "    past_con = []\n",
    "    past_answerCode = []\n",
    "    prsnt_cat = []\n",
    "    prsnt_con = []\n",
    "    prsnt_answerCode = []\n",
    "\n",
    "    for sample in samples:\n",
    "        past_cat += [pad_sequence(sample['past_cat'] + 1, max_len = max_len, padding_value = 0)]\n",
    "        past_con += [pad_sequence(sample['past_con'], max_len = max_len, padding_value = 0)]\n",
    "        past_answerCode += [pad_sequence(sample['past_answerCode'] + 1, max_len = max_len, padding_value = 0)]\n",
    "        prsnt_cat += [pad_sequence(sample['prsnt_cat'] + 1, max_len = max_len, padding_value = 0)]\n",
    "        prsnt_con += [pad_sequence(sample['prsnt_con'], max_len = max_len, padding_value = 0)]\n",
    "        prsnt_answerCode += [pad_sequence(sample['prsnt_answerCode'], max_len = max_len, padding_value = -1)]\n",
    "\n",
    "    return torch.tensor(past_cat, dtype = torch.long), torch.tensor(past_con, dtype = torch.float32), torch.tensor(past_answerCode, dtype = torch.long), torch.tensor(prsnt_cat, dtype = torch.long), torch.tensor(prsnt_con, dtype = torch.float32), torch.tensor(prsnt_answerCode, dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Transformer + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, dropout_ratio):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        score = torch.matmul(q, k.permute(0,1,3,2)) / math.sqrt(self.hidden_dim)\n",
    "        score = score.masked_fill(mask==0, -1e10)\n",
    "        att_d = self.dropout(F.softmax(score, dim=-1))\n",
    "        output = torch.matmul(att_d, v)\n",
    "        return output, att_d\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, hidden_dim, dropout_ratio):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "        self.head_dim = self.hidden_dim // self.n_heads\n",
    "        self.attention = ScaledDotProductAttention(hidden_dim, dropout_ratio)\n",
    "        self.layerNorm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        self.w_q = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.w_k = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.w_v = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.w_o = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, src, mask):\n",
    "\n",
    "        resicnect = src\n",
    "        batch_size, seq_len = src.size(0), src.size(1)\n",
    "        q = self.w_q(src).view(batch_size, seq_len, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
    "        k = self.w_k(src).view(batch_size, seq_len, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
    "        v = self.w_v(src).view(batch_size, seq_len, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
    "        \n",
    "        \n",
    "        output, att_d = self.attention(q, k, v, mask)\n",
    "        output = output.transpose(1,2).contiguous()\n",
    "        output = output.view(batch_size, seq_len, -1)\n",
    "\n",
    "        output = self.layerNorm(self.dropout(self.w_o(output)) + resicnect)\n",
    "\n",
    "        return output, att_d\n",
    "\n",
    "\n",
    "class PositionWiseFeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, hidden_dim, dropout_ratio):\n",
    "        super().__init__()\n",
    "        self.layerNorm = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "        self.fc_1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        resicnect = x\n",
    "        output = self.fc_2(torch.relu(self.dropout(self.fc_1(x))))\n",
    "        output = self.layerNorm(self.dropout(output)+ resicnect)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class SasRec(nn.Module):\n",
    "    def __init__(self, n_heads, hidden_dim, dropout_ratio):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(n_heads, hidden_dim, dropout_ratio)\n",
    "        self.FFN = PositionWiseFeedForwardNetwork(hidden_dim, dropout_ratio)\n",
    "\n",
    "    def forward(self, src, mask):\n",
    "        output, att_d = self.attention(src, mask)\n",
    "        output = self.FFN(output)\n",
    "        return output, att_d\n",
    "\n",
    "\n",
    "class sasreclstm(nn.Module):\n",
    "    def __init__(self, \n",
    "                 n_assID, \n",
    "                 n_testID, \n",
    "                 n_tag, \n",
    "                 n_level, \n",
    "                 n_dayweek,\n",
    "                 con_cols, \n",
    "                 cat_cols, \n",
    "                 hidden_dim, \n",
    "                 emb_size, \n",
    "                 n_heads, \n",
    "                 n_layers, \n",
    "                 dropout_ratio, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_assID = n_assID\n",
    "        self.n_testID = n_testID\n",
    "        self.n_tag = n_tag\n",
    "        self.n_level = n_level\n",
    "        self.n_dayweek = n_dayweek\n",
    "        self.con_cols = con_cols\n",
    "        self.cat_cols = cat_cols\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.emb_size = emb_size\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "        self.device = device\n",
    "\n",
    "        past_embd = {}\n",
    "        past_embd['assessmentItemID_'] = nn.Embedding(self.n_assID + 1, self.emb_size, padding_idx=0)\n",
    "        past_embd['testId_'] = nn.Embedding(self.n_testID + 1, self.emb_size, padding_idx=0)\n",
    "        past_embd['KnowledgeTag_'] = nn.Embedding(self.n_tag + 1, self.emb_size, padding_idx=0)\n",
    "        past_embd['level_'] = nn.Embedding(self.n_level + 1, self.emb_size, padding_idx= 0)\n",
    "        past_embd['dayofweek'] = nn.Embedding(self.n_dayweek + 1, self.emb_size, padding_idx=0)\n",
    "\n",
    "\n",
    "        self.past_embd_dict = nn.ModuleDict(past_embd)\n",
    "\n",
    "        self.past_answerCode_embd = nn.Embedding(3, self.hidden_dim, padding_idx=0)\n",
    "\n",
    "        self.past_embd_cat = nn.Sequential(nn.Linear(len(cat_cols) * self.emb_size, self.hidden_dim // 2), nn.LayerNorm(self.hidden_dim // 2))\n",
    "\n",
    "        self.past_embd_con = nn.Sequential(nn.Linear(len(con_cols),self.hidden_dim// 2), nn.LayerNorm(self.hidden_dim //2))\n",
    "\n",
    "        self.embd_layernorm = nn.LayerNorm(self.hidden_dim)\n",
    "\n",
    "        self.past_lstm = nn.LSTM(\n",
    "            input_size = self.hidden_dim,\n",
    "            hidden_size = self.hidden_dim,\n",
    "            num_layers = self.n_layers,\n",
    "            batch_first = True,\n",
    "            bidirectional = False,\n",
    "            dropout = dropout_ratio\n",
    "        )\n",
    "        \n",
    "        self.past_blocks = nn.ModuleList([SasRec(self.n_heads, self.hidden_dim, dropout_ratio) for _ in range(self.n_layers)])\n",
    "\n",
    "\n",
    "        prsnt_embd = {}\n",
    "        prsnt_embd['assessmentItemID_'] = nn.Embedding(self.n_assID + 1, self.emb_size, padding_idx=0)\n",
    "        prsnt_embd['testId_'] = nn.Embedding(self.n_testID + 1, self.emb_size, padding_idx=0)\n",
    "        prsnt_embd['KnowledgeTag_'] = nn.Embedding(self.n_tag + 1, self.emb_size, padding_idx=0)\n",
    "        prsnt_embd['level_'] = nn.Embedding(self.n_level + 1, self.emb_size, padding_idx= 0)\n",
    "        prsnt_embd['dayofweek'] = nn.Embedding(self.n_dayweek + 1, self.emb_size, padding_idx=0)\n",
    "\n",
    "        self.prsnt_embd_dict = nn.ModuleDict(prsnt_embd)\n",
    "\n",
    "        self.prsnt_embd_cat = nn.Sequential(nn.Linear(len(cat_cols) * self.emb_size, self.hidden_dim // 2), nn.LayerNorm(self.hidden_dim // 2))\n",
    "\n",
    "        self.prsnt_embd_con = nn.Sequential(nn.Linear(len(con_cols),self.hidden_dim// 2), nn.LayerNorm(self.hidden_dim //2))\n",
    "\n",
    "        self.prsnt_lstm = nn.LSTM(\n",
    "            input_size = self.hidden_dim,\n",
    "            hidden_size = self.hidden_dim,\n",
    "            num_layers = self.n_layers,\n",
    "            batch_first = True,\n",
    "            bidirectional = False,\n",
    "            dropout = dropout_ratio\n",
    "        )\n",
    "\n",
    "        self.prsnt_blocks = nn.ModuleList([SasRec(self.n_heads, self.hidden_dim, dropout_ratio) for _ in range(self.n_layers)])\n",
    "\n",
    "        self.predict_layer = nn.Sequential(nn.Linear(self.hidden_dim*2, 1), nn.Sigmoid())\n",
    "\n",
    "\n",
    "    def forward(self, past_cat, past_con, past_answerCode, prsnt_cat, prsnt_con):\n",
    "\n",
    "        mask_pad = torch.BoolTensor(past_answerCode > 0).unsqueeze(1).unsqueeze(1)\n",
    "        mask_time = (1 - torch.triu(torch.ones((1,1,past_answerCode.size(1))), diagonal=1)).bool()\n",
    "        mask = (mask_pad & mask_time).to(self.device)\n",
    "\n",
    "\n",
    "        past_embd_cat_lst = []\n",
    "        for i, cat in enumerate(self.cat_cols):\n",
    "            past_embd_cat_lst.append(self.past_embd_dict[cat](past_cat[:,:,i]))\n",
    "\n",
    "    \n",
    "        past_cat_embd = torch.concat(past_embd_cat_lst, dim=-1)\n",
    "        past_cat_embd = self.past_embd_cat(past_cat_embd)\n",
    "        past_con_embd = self.past_embd_con(past_con)\n",
    "\n",
    "        past_embd = torch.concat([past_cat_embd, past_con_embd], dim=-1)\n",
    "        past_embd += self.past_answerCode_embd(past_answerCode.to(self.device))\n",
    "        past_embd = self.embd_layernorm(past_embd)\n",
    "\n",
    "        for b in self.past_blocks:\n",
    "            past_embd, _ = b(past_embd, mask)\n",
    "        \n",
    "        past_embd, _ = self.past_lstm(past_embd)\n",
    "\n",
    "\n",
    "\n",
    "        prsnt_embd_cat_lst = []\n",
    "\n",
    "        for i,cat in enumerate(self.cat_cols):\n",
    "            prsnt_embd_cat_lst.append(self.prsnt_embd_dict[cat](prsnt_cat[:,:,i]))\n",
    "\n",
    "        prsnt_cat_embd = torch.concat(prsnt_embd_cat_lst, dim=-1)\n",
    "        prsnt_cat_embd = self.prsnt_embd_cat(prsnt_cat_embd)\n",
    "        prsnt_con_embd = self.prsnt_embd_con(prsnt_con)\n",
    "\n",
    "        prsnt_embd = torch.concat([prsnt_cat_embd, prsnt_con_embd], dim=-1)\n",
    "        \n",
    "        for b in self.prsnt_blocks:\n",
    "            prsnt_embd, _ = b(prsnt_embd, mask)\n",
    "        \n",
    "        prsnt_embd, _ = self.prsnt_lstm(prsnt_embd)\n",
    "\n",
    "\n",
    "        embd = torch.concat([self.dropout(past_embd), self.dropout(prsnt_embd)], dim =-1)\n",
    "\n",
    "        output = self.predict_layer(embd)\n",
    "\n",
    "        return output\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    loss_val = 0\n",
    "\n",
    "    for past_cat, past_con,past_answerCode,prsnt_cat,prsnt_con,prsnt_answerCode in data_loader:\n",
    "        past_cat, past_con = past_cat.to(device), past_con.to(device)\n",
    "        prsnt_cat,prsnt_con,prsnt_answerCode = prsnt_cat.to(device), prsnt_con.to(device), prsnt_answerCode.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(past_cat, past_con,past_answerCode,prsnt_cat,prsnt_con).squeeze(2)\n",
    "        loss = criterion(output[prsnt_answerCode != -1], prsnt_answerCode[prsnt_answerCode != -1])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_val += loss.item()\n",
    "\n",
    "    loss_val /= len(data_loader)\n",
    "\n",
    "    return loss_val\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "\n",
    "    target = []\n",
    "    pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for past_cat, past_con,past_answerCode,prsnt_cat,prsnt_con,prsnt_answerCode in data_loader:\n",
    "\n",
    "            past_cat, past_con = past_cat.to(device), past_con.to(device)\n",
    "            prsnt_cat,prsnt_con,prsnt_answerCode = prsnt_cat.to(device), prsnt_con.to(device), prsnt_answerCode.to(device)\n",
    "\n",
    "            output = model(past_cat, past_con,past_answerCode,prsnt_cat,prsnt_con).squeeze(2)\n",
    "\n",
    "            target.extend(prsnt_answerCode[:,-1].cpu().numpy().tolist())\n",
    "            pred.extend(output[:,-1].cpu().numpy().tolist())\n",
    "    \n",
    "    roc_auc = roc_auc_score(target, pred)\n",
    "\n",
    "    return roc_auc\n",
    "\n",
    "\n",
    "def predict(model, data_loader):\n",
    "    model.eval()\n",
    "\n",
    "    pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for past_cat, past_con,past_answerCode,prsnt_cat,prsnt_con,prsnt_answerCode in data_loader:\n",
    "            past_cat, past_con = past_cat.to(device), past_con.to(device)\n",
    "            prsnt_cat,prsnt_con = prsnt_cat.to(device), prsnt_con.to(device)\n",
    "            output = model(past_cat, past_con,past_answerCode,prsnt_cat,prsnt_con).squeeze(2)\n",
    "\n",
    "            pred.extend(output[:,-1].cpu().numpy().tolist())\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "HIDDEN_DIM = 128\n",
    "EMB_SIZE = 64\n",
    "N_HEADS = 2\n",
    "N_LAYERS = 1\n",
    "DROPOUT_RATIO = 0.3\n",
    "NUM_WORKERS = 8\n",
    "\n",
    "\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "SEED = 4444\n",
    "\n",
    "DATA_PATH = '/opt/ml/ephemeral/data'\n",
    "MODEL_PATH = '/opt/ml/ephemeral/model'\n",
    "SUBMISSION_PATH = '/opt/ml/ephemeral/submission'\n",
    "\n",
    "model_name = 'sasrec+lstm.pt'\n",
    "submission_name = 'sasrec+lstm.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(MODEL_PATH):\n",
    "    os.mkdir(MODEL_PATH)\n",
    "if not os.path.isdir(SUBMISSION_PATH):\n",
    "    os.mkdir(SUBMISSION_PATH)\n",
    "\n",
    "dataset_ = DTset(DATA_PATH=DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['elapsed', 'answerCode_rate', 'elapsed_std']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_.con_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['assessmentItemID_', 'testId_', 'KnowledgeTag_', 'level_', 'dayofweek']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_.cat_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d3f525ab90f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mn_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_LAYERS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mdropout_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDROPOUT_RATIO\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         device = device).to(device)\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m/opt/conda/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    895\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    896\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 897\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "oof_roc_auc = 0\n",
    "\n",
    "for oof in dataset_.oof_ID_set.keys():\n",
    "    train_df,valid_df = dataset_.get_oof(oof)\n",
    "\n",
    "    seed_everything(4444+ oof)\n",
    "\n",
    "    train_dataset = Custom(df = train_df,)\n",
    "    train_data_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "        collate_fn = train_make_batch,\n",
    "        num_workers=NUM_WORKERS)\n",
    "    \n",
    "    valid_dataset = Custom(df = valid_df,)\n",
    "    valid_data_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size = 1,\n",
    "        shuffle = False, \n",
    "        drop_last = False,\n",
    "        collate_fn = train_make_batch,\n",
    "        num_workers = NUM_WORKERS)\n",
    "\n",
    "    model = sasreclstm(\n",
    "        n_assID=dataset_.n_assItemID_,\n",
    "        n_testID=dataset_.n_testId_,\n",
    "        n_tag=dataset_.n_tag_,\n",
    "        n_level=dataset_.n_level_,\n",
    "        n_dayweek=dataset_.n_dayweek,\n",
    "        con_cols=dataset_.con_cols,\n",
    "        cat_cols=dataset_.cat_cols,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        emb_size=EMB_SIZE,\n",
    "        n_heads=N_HEADS,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout_ratio=DROPOUT_RATIO,\n",
    "        device = device).to(device)\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    best_epoch = 0\n",
    "    best_train_loss = 0\n",
    "    best_roc_auc = 0\n",
    "\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "\n",
    "        for _ in range(1):\n",
    "            train_loss = train(model = model, data_loader = train_data_loader, criterion = criterion, optimizer = optimizer)\n",
    "            roc_auc = evaluate(model = model, data_loader = valid_data_loader)\n",
    "            if best_roc_auc < roc_auc:\n",
    "                best_epoch = epoch\n",
    "                best_train_loss = train_loss\n",
    "                best_roc_auc = roc_auc\n",
    "                torch.save(model.state_dict(), os.path.join(MODEL_PATH, f'oof_{oof}_' + model_name))\n",
    "\n",
    "    \n",
    "    print(f'BEST OOF-{oof}| Epoch: {best_epoch:3d}| Train loss: {best_train_loss:.5f}| roc_auc: {best_roc_auc:.5f}')\n",
    "\n",
    "    oof_roc_auc += best_roc_auc\n",
    "\n",
    "print(f'Total roc_auc: {oof_roc_auc / len(dataset_.oof_ID_set.keys()):.5f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = dataset_.get_test_data()\n",
    "test_dataset = Custom(df = test_df,)\n",
    "test_data_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size = 1, \n",
    "    shuffle = False, \n",
    "    drop_last = False,\n",
    "    collate_fn = train_make_batch,\n",
    "    num_workers = NUM_WORKERS)\n",
    "\n",
    "pred_list = []\n",
    "\n",
    "model = sasreclstm(\n",
    "    n_assID=dataset_.n_assItemID_,\n",
    "    n_testID=dataset_.n_testId_,\n",
    "    n_tag=dataset_.n_tag_,\n",
    "    n_level=dataset_.n_level_,\n",
    "    n_dayweek=dataset_.n_dayweek,\n",
    "    con_cols=dataset_.con_cols,\n",
    "    cat_cols=dataset_.cat_cols,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    emb_size=EMB_SIZE,\n",
    "    n_heads=N_HEADS,\n",
    "    n_layers=N_LAYERS,\n",
    "    dropout_ratio=DROPOUT_RATIO,\n",
    "    device = device\n",
    ").to(device)\n",
    "\n",
    "#    n_student_level = dataset_.n_student_level_,\n",
    "    #n_beta = dataset_.n_beta_,\n",
    "\n",
    "for oof in dataset_.oof_ID_set.keys():\n",
    "    model.load_state_dict(torch.load(os.path.join(MODEL_PATH, f'oof_{oof}_' + model_name)))\n",
    "    pred = predict(model = model, data_loader = test_data_loader)\n",
    "    pred_list.append(pred)\n",
    "\n",
    "pred_list = np.array(pred_list).mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(data = np.array(pred_list), columns = ['prediction'])\n",
    "submission['id'] = submission.index\n",
    "submission = submission[['id', 'prediction']]\n",
    "submission.to_csv(os.path.join(SUBMISSION_PATH, 'OOF-Ensemble-' + submission_name), index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dkt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
