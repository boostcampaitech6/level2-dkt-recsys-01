{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py36/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "import gc \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/opt/ml/ephemeral/data/\"\n",
    "train_df = pd.read_csv(path + \"train_data.csv\")\n",
    "test_df = pd.read_csv(path + \"test_data.csv\")\n",
    "submit_df = pd.read_csv(path + \"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTset():\n",
    "    def __init__(self, DATA_PATH):\n",
    "        self.preprocessing(DATA_PATH)\n",
    "        self.oof_ID_set = self.split_data()\n",
    "    \n",
    "    def split_data(self):\n",
    "        user_list = self.df['userID'].unique().tolist()\n",
    "        oof_ID_set = {}\n",
    "        kfold = KFold(n_splits = 5, shuffle = True, random_state = 4444)\n",
    "        for i, (t, v) in enumerate(kfold.split(user_list)):\n",
    "            oof_ID_set[i] = v.tolist()\n",
    "        \n",
    "        return oof_ID_set\n",
    "    \n",
    "    def preprocessing(self, DATA_PATH):\n",
    "\n",
    "        dtype = {\n",
    "            'userID': 'int16',\n",
    "            'answerCode': 'int8',\n",
    "            'KnowledgeTag': 'int16'\n",
    "        }\n",
    "        \n",
    "        train_df = pd.read_csv(os.path.join(DATA_PATH, 'train_data.csv'), dtype=dtype, parse_dates=['Timestamp'])\n",
    "        test_df = pd.read_csv(os.path.join(DATA_PATH, 'test_data.csv'), dtype=dtype, parse_dates=['Timestamp'])\n",
    "\n",
    "\n",
    "\n",
    "        def FE(df: pd.DataFrame) -> pd.DataFrame:\n",
    "            def chg_idx(lst):\n",
    "                tmp = {}\n",
    "                for i,j in enumerate(lst):\n",
    "                    tmp[j] = i\n",
    "                return tmp\n",
    "\n",
    "            def convert_time(s: str):\n",
    "                timestamp = time.mktime(s.timetuple())\n",
    "                return int(timestamp)\n",
    "\n",
    "            df[\"convert_time\"] = df[\"Timestamp\"].apply(convert_time)\n",
    "            df['level'] = df['testId'].apply(lambda x: x[2])\n",
    "            df['shift'] = df['convert_time'].shift(-1).fillna(0).astype(int)\n",
    "            df['elapsed'] = df['shift'] - df['convert_time']\n",
    "            df['check'] = df['userID'].shift(-1)\n",
    "            df['d_check'] = df['testId'].shift(-1)\n",
    "            df.loc[(df['userID'] != df['check']) | (df['testId'] != df['d_check']) | (df['elapsed'] >= 86400), 'elapsed'] = 0\n",
    "            df = df.drop(['shift','check','d_check','convert_time'], axis=1)\n",
    "            # df['Timestamp'] = df['Timestamp'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
    "            df['dayofweek'] = df['Timestamp'].dt.dayofweek\n",
    "            answer_rate = df.groupby('assessmentItemID')['answerCode'].mean()\n",
    "            df = df.join(answer_rate, on='assessmentItemID', rsuffix='_rate')\n",
    "\n",
    "            col_ = {}\n",
    "\n",
    "            assItemID_ = chg_idx(df['assessmentItemID'].unique().tolist())\n",
    "            df['assessmentItemID_'] = df['assessmentItemID'].apply(lambda x: assItemID_[x])\n",
    "            col_['assItemID_'] = assItemID_\n",
    "\n",
    "            testID_ = chg_idx(df['testId'].unique().tolist())\n",
    "            df['testId_'] = df['testId'].apply(lambda x: testID_[x])\n",
    "            col_['testID_'] = testID_\n",
    "            \n",
    "            tag_ = chg_idx(df['KnowledgeTag'].unique().tolist())\n",
    "            df['KnowledgeTag_'] = df['KnowledgeTag'].apply(lambda x: tag_[x])\n",
    "            col_['tag_'] = tag_\n",
    "\n",
    "            level_ = chg_idx(df['level'].unique().tolist())\n",
    "            df['level_'] = df['level'].apply(lambda x: level_[x])\n",
    "            col_['level_'] = level_\n",
    "\n",
    "\n",
    "            return df, col_\n",
    "\n",
    "\n",
    "        train_df, col_ = FE(train_df)\n",
    "        test_df, _  = FE(test_df)\n",
    "\n",
    "        self.assItemID_ = col_['assItemID_']\n",
    "        \n",
    "        self.n_assItemID_ = len(col_['assItemID_'])\n",
    "        self.n_testId_ = len(col_['testID_'])\n",
    "        self.n_tag_ = len(col_['tag_'])\n",
    "        self.n_level_ = len(col_['level_'])\n",
    "        self.n_dayweek = 7\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.df = pd.concat([train_df, test_df[test_df['answerCode'] != -1]]).reset_index(drop=True)\n",
    "        self.cat_cols = ['assessmentItemID_','testId_','KnowledgeTag_','level_','dayofweek']\n",
    "        self.con_cols = ['elapsed','answerCode_rate']\n",
    "\n",
    "    def get_oof(self, oof):\n",
    "        oof_ID_set_v = self.oof_ID_set[oof]\n",
    "\n",
    "        train = []\n",
    "        valid = []\n",
    "\n",
    "        grob = self.df.groupby('userID')\n",
    "        for usr, df in grob:\n",
    "            if usr in oof_ID_set_v:\n",
    "                train.append(df.iloc[:-1,:])\n",
    "                valid.append(df.copy())\n",
    "            else:\n",
    "                train.append(df)\n",
    "\n",
    "        train = pd.concat(train).reset_index(drop = True)\n",
    "        valid = pd.concat(valid).reset_index(drop = True)\n",
    "\n",
    "        return train, valid\n",
    "    \n",
    "    def get_test_data(self):\n",
    "        return self.test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom(DTset):\n",
    "    def __init__(self,\n",
    "                 df,\n",
    "                 cat_cols = ['assessmentItemID_','testId_','KnowledgeTag_','level_','dayofweek'],\n",
    "                 con_cols = ['elapsed','answerCode_rate']):\n",
    "        \n",
    "        self.cat_cols = cat_cols\n",
    "        self.con_cols = con_cols\n",
    "        self.get_df = df.groupby('userID')\n",
    "        self.user_lst = df['userID'].unique().tolist()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.user_lst)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        user = self.user_lst[idx]\n",
    "        get_df = self.get_df.get_group(user)\n",
    "\n",
    "        prsnt_df = get_df.iloc[1:,:]\n",
    "        prsnt_cat = prsnt_df[self.cat_cols].values\n",
    "        prsnt_con = prsnt_df[self.con_cols].values\n",
    "        prsnt_answerCode = prsnt_df['answerCode'].values\n",
    "\n",
    "        past_df = get_df.iloc[:-1,:]\n",
    "        past_cat = past_df[self.cat_cols].values\n",
    "        past_con = past_df[self.con_cols].values\n",
    "        past_answerCode = past_df['answerCode'].values\n",
    "\n",
    "        return {'past_cat' : past_cat,\n",
    "                'past_con' : past_con,\n",
    "                'past_answerCode' : past_answerCode,\n",
    "                'prsnt_cat' : prsnt_cat,\n",
    "                'prsnt_con' : prsnt_con,\n",
    "                'prsnt_answerCode' : prsnt_answerCode}\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pad_sequence(seq, max_len, padding_value = 0):\n",
    "    try:\n",
    "        seq_len, col = seq.shape\n",
    "        padding = np.zeros((max_len - seq_len, col)) + padding_value\n",
    "    except:\n",
    "        seq_len = seq.shape[0]\n",
    "        padding = np.zeros((max_len - seq_len, )) + padding_value\n",
    "\n",
    "    padding_seq = np.concatenate([padding, seq])\n",
    "\n",
    "    return padding_seq\n",
    "\n",
    "def train_make_batch(samples):\n",
    "    max_len = 0\n",
    "    for sample in samples:\n",
    "        seq_len, _ = sample['past_cat'].shape\n",
    "        if max_len < seq_len:\n",
    "            max_len = seq_len\n",
    "    \n",
    "    past_cat = []\n",
    "    past_con = []\n",
    "    past_answerCode = []\n",
    "    prsnt_cat = []\n",
    "    prsnt_con = []\n",
    "    prsnt_answerCode = []\n",
    "\n",
    "    for sample in samples:\n",
    "        past_cat += [pad_sequence(sample['past_cat'] + 1, max_len = max_len, padding_value = 0)]\n",
    "        past_con += [pad_sequence(sample['past_con'], max_len = max_len, padding_value = 0)]\n",
    "        past_answerCode += [pad_sequence(sample['past_answerCode'] + 1, max_len = max_len, padding_value = 0)]\n",
    "        prsnt_cat += [pad_sequence(sample['prsnt_cat'] + 1, max_len = max_len, padding_value = 0)]\n",
    "        prsnt_con += [pad_sequence(sample['prsnt_con'], max_len = max_len, padding_value = 0)]\n",
    "        prsnt_answerCode += [pad_sequence(sample['prsnt_answerCode'], max_len = max_len, padding_value = -1)]\n",
    "\n",
    "    return torch.tensor(past_cat, dtype = torch.long), torch.tensor(past_con, dtype = torch.float32), torch.tensor(past_answerCode, dtype = torch.long), torch.tensor(prsnt_cat, dtype = torch.long), torch.tensor(prsnt_con, dtype = torch.float32), torch.tensor(prsnt_answerCode, dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Transformer + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, dropout_ratio):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        score = torch.matmul(q, k.permute(0,1,3,2)) / math.sqrt(self.hidden_dim)\n",
    "        score = score.masked_fill(mask==0, -1e10)\n",
    "        att_d = self.dropout(F.softmax(score, dim=-1))\n",
    "        output = torch.matmul(att_d, v)\n",
    "        return output, att_d\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, hidden_dim, dropout_ratio):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "        self.head_dim = self.hidden_dim // self.n_heads\n",
    "        self.attention = ScaledDotProductAttention(hidden_dim, dropout_ratio)\n",
    "        self.layerNorm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        self.w_q = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.w_k = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.w_v = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.w_o = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, src, mask):\n",
    "\n",
    "        resicnect = src\n",
    "        batch_size, seq_len = src.size(0), src.size(1)\n",
    "        q = self.w_q(src).view(batch_size, seq_len, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
    "        k = self.w_k(src).view(batch_size, seq_len, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
    "        v = self.w_v(src).view(batch_size, seq_len, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
    "        \n",
    "        \n",
    "        output, att_d = self.attention(q, k, v, mask)\n",
    "        output = output.transpose(1,2).contiguous()\n",
    "        output = output.view(batch_size, seq_len, -1)\n",
    "\n",
    "        output = self.layerNorm(self.dropout(self.w_o(output)) + resicnect)\n",
    "\n",
    "        return output, att_d\n",
    "\n",
    "\n",
    "class PositionWiseFeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, hidden_dim, dropout_ratio):\n",
    "        super().__init__()\n",
    "        self.layerNorm = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "        self.fc_1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        resicnect = x\n",
    "        output = self.fc_2(torch.relu(self.dropout(self.fc_1(x))))\n",
    "        output = self.layerNorm(self.dropout(output)+ resicnect)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class SasRec(nn.Module):\n",
    "    def __init__(self, n_heads, hidden_dim, dropout_ratio):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(n_heads, hidden_dim, dropout_ratio)\n",
    "        self.FFN = PositionWiseFeedForwardNetwork(hidden_dim, dropout_ratio)\n",
    "\n",
    "    def forward(self, src, mask):\n",
    "        output, att_d = self.attention(src, mask)\n",
    "        output = self.FFN(output)\n",
    "        return output, att_d\n",
    "\n",
    "\n",
    "class sasreclstm(nn.Module):\n",
    "    def __init__(self, n_assID, n_testID, n_tag, n_level, n_dayweek, con_cols, cat_cols, hidden_dim, emb_size, n_heads, n_layers, dropout_ratio, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_assID = n_assID\n",
    "        self.n_testID = n_testID\n",
    "        self.n_tag = n_tag\n",
    "        self.n_level = n_level\n",
    "        self.n_dayweek = n_dayweek\n",
    "        self.con_cols = con_cols\n",
    "        self.cat_cols = cat_cols\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.emb_size = emb_size\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "        self.device = device\n",
    "\n",
    "        past_embd = {}\n",
    "        past_embd['assessmentItemID_'] = nn.Embedding(self.n_assID + 1, self.emb_size, padding_idx=0)\n",
    "        past_embd['testId_'] = nn.Embedding(self.n_testID + 1, self.emb_size, padding_idx=0)\n",
    "        past_embd['KnowledgeTag_'] = nn.Embedding(self.n_tag + 1, self.emb_size, padding_idx=0)\n",
    "        past_embd['level_'] = nn.Embedding(self.n_level + 1, self.emb_size, padding_idx= 0)\n",
    "        past_embd['dayofweek'] = nn.Embedding(self.n_dayweek + 1, self.emb_size, padding_idx=0)\n",
    "\n",
    "        self.past_embd_dict = nn.ModuleDict(past_embd)\n",
    "\n",
    "        self.past_answerCode_embd = nn.Embedding(3, self.hidden_dim, padding_idx=0)\n",
    "\n",
    "        self.past_embd_cat = nn.Sequential(nn.Linear(5 * self.emb_size, self.hidden_dim // 2), nn.LayerNorm(self.hidden_dim // 2))\n",
    "\n",
    "        self.past_embd_con = nn.Sequential(nn.Linear(2,self.hidden_dim// 2), nn.LayerNorm(self.hidden_dim //2))\n",
    "\n",
    "        self.embd_layernorm = nn.LayerNorm(self.hidden_dim)\n",
    "\n",
    "        self.past_lstm = nn.LSTM(\n",
    "            input_size = self.hidden_dim,\n",
    "            hidden_size = self.hidden_dim,\n",
    "            num_layers = self.n_layers,\n",
    "            batch_first = True,\n",
    "            bidirectional = False,\n",
    "            dropout = dropout_ratio\n",
    "        )\n",
    "        \n",
    "        self.past_blocks = nn.ModuleList([SasRec(self.n_heads, self.hidden_dim, dropout_ratio) for _ in range(self.n_layers)])\n",
    "\n",
    "\n",
    "        prsnt_embd = {}\n",
    "        prsnt_embd['assessmentItemID_'] = nn.Embedding(self.n_assID + 1, self.emb_size, padding_idx=0)\n",
    "        prsnt_embd['testId_'] = nn.Embedding(self.n_testID + 1, self.emb_size, padding_idx=0)\n",
    "        prsnt_embd['KnowledgeTag_'] = nn.Embedding(self.n_tag + 1, self.emb_size, padding_idx=0)\n",
    "        prsnt_embd['level_'] = nn.Embedding(self.n_level + 1, self.emb_size, padding_idx= 0)\n",
    "        prsnt_embd['dayofweek'] = nn.Embedding(self.n_dayweek + 1, self.emb_size, padding_idx=0)\n",
    "\n",
    "        self.prsnt_embd_dict = nn.ModuleDict(prsnt_embd)\n",
    "\n",
    "        self.prsnt_embd_cat = nn.Sequential(nn.Linear(5 * self.emb_size, self.hidden_dim // 2), nn.LayerNorm(self.hidden_dim // 2))\n",
    "\n",
    "        self.prsnt_embd_con = nn.Sequential(nn.Linear(2,self.hidden_dim// 2), nn.LayerNorm(self.hidden_dim //2))\n",
    "\n",
    "        self.prsnt_lstm = nn.LSTM(\n",
    "            input_size = self.hidden_dim,\n",
    "            hidden_size = self.hidden_dim,\n",
    "            num_layers = self.n_layers,\n",
    "            batch_first = True,\n",
    "            bidirectional = False,\n",
    "            dropout = dropout_ratio\n",
    "        )\n",
    "\n",
    "        self.prsnt_blocks = nn.ModuleList([SasRec(self.n_heads, self.hidden_dim, dropout_ratio) for _ in range(self.n_layers)])\n",
    "\n",
    "        self.predict_layer = nn.Sequential(nn.Linear(self.hidden_dim*2, 1), nn.Sigmoid())\n",
    "\n",
    "\n",
    "    def forward(self, past_cat, past_con, past_answerCode, prsnt_cat, prsnt_con):\n",
    "\n",
    "        mask_pad = torch.BoolTensor(past_answerCode > 0).unsqueeze(1).unsqueeze(1)\n",
    "        mask_time = (1 - torch.triu(torch.ones((1,1,past_answerCode.size(1))), diagonal=1)).bool()\n",
    "        mask = (mask_pad & mask_time).to(self.device)\n",
    "\n",
    "\n",
    "        past_embd_cat_lst = []\n",
    "        for i, cat in enumerate(self.cat_cols):\n",
    "            past_embd_cat_lst.append(self.past_embd_dict[cat](past_cat[:,:,i]))\n",
    "\n",
    "        \n",
    "        past_cat_embd = torch.concat(past_embd_cat_lst, dim=-1)\n",
    "        past_cat_embd = self.past_embd_cat(past_cat_embd)\n",
    "        past_con_embd = self.past_embd_con(past_con)\n",
    "\n",
    "        past_embd = torch.concat([past_cat_embd, past_con_embd], dim=-1)\n",
    "        past_embd += self.past_answerCode_embd(past_answerCode.to(self.device))\n",
    "        past_embd = self.embd_layernorm(past_embd)\n",
    "\n",
    "        for b in self.past_blocks:\n",
    "            past_embd, _ = b(past_embd, mask)\n",
    "        \n",
    "        past_embd, _ = self.past_lstm(past_embd)\n",
    "\n",
    "\n",
    "\n",
    "        prsnt_embd_cat_lst = []\n",
    "\n",
    "        for i,cat in enumerate(self.cat_cols):\n",
    "            prsnt_embd_cat_lst.append(self.prsnt_embd_dict[cat](prsnt_cat[:,:,i]))\n",
    "\n",
    "        prsnt_cat_embd = torch.concat(prsnt_embd_cat_lst, dim=-1)\n",
    "        prsnt_cat_embd = self.prsnt_embd_cat(prsnt_cat_embd)\n",
    "        prsnt_con_embd = self.prsnt_embd_con(prsnt_con)\n",
    "\n",
    "        prsnt_embd = torch.concat([prsnt_cat_embd, prsnt_con_embd], dim=-1)\n",
    "        \n",
    "        for b in self.prsnt_blocks:\n",
    "            prsnt_embd, _ = b(prsnt_embd, mask)\n",
    "        \n",
    "        prsnt_embd, _ = self.prsnt_lstm(prsnt_embd)\n",
    "\n",
    "\n",
    "        embd = torch.concat([self.dropout(past_embd), self.dropout(prsnt_embd)], dim =-1)\n",
    "\n",
    "        output = self.predict_layer(embd)\n",
    "\n",
    "        return output\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    loss_val = 0\n",
    "\n",
    "    for past_cat, past_con,past_answerCode,prsnt_cat,prsnt_con,prsnt_answerCode in data_loader:\n",
    "        past_cat, past_con = past_cat.to(device), past_con.to(device)\n",
    "        prsnt_cat,prsnt_con,prsnt_answerCode = prsnt_cat.to(device), prsnt_con.to(device), prsnt_answerCode.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(past_cat, past_con,past_answerCode,prsnt_cat,prsnt_con).squeeze(2)\n",
    "        loss = criterion(output[prsnt_answerCode != -1], prsnt_answerCode[prsnt_answerCode != -1])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_val += loss.item()\n",
    "\n",
    "    loss_val /= len(data_loader)\n",
    "\n",
    "    return loss_val\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "\n",
    "    target = []\n",
    "    pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for past_cat, past_con,past_answerCode,prsnt_cat,prsnt_con,prsnt_answerCode in data_loader:\n",
    "\n",
    "            past_cat, past_con = past_cat.to(device), past_con.to(device)\n",
    "            prsnt_cat,prsnt_con,prsnt_answerCode = prsnt_cat.to(device), prsnt_con.to(device), prsnt_answerCode.to(device)\n",
    "\n",
    "            output = model(past_cat, past_con,past_answerCode,prsnt_cat,prsnt_con).squeeze(2)\n",
    "\n",
    "            target.extend(prsnt_answerCode[:,-1].cpu().numpy().tolist())\n",
    "            pred.extend(output[:,-1].cpu().numpy().tolist())\n",
    "    \n",
    "    roc_auc = roc_auc_score(target, pred)\n",
    "\n",
    "    return roc_auc\n",
    "\n",
    "\n",
    "def predict(model, data_loader):\n",
    "    model.eval()\n",
    "\n",
    "    pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for past_cat, past_con,past_answerCode,prsnt_cat,prsnt_con,prsnt_answerCode in data_loader:\n",
    "            past_cat, past_con = past_cat.to(device), past_con.to(device)\n",
    "            prsnt_cat,prsnt_con = prsnt_cat.to(device), prsnt_con.to(device)\n",
    "            output = model(past_cat, past_con,past_answerCode,prsnt_cat,prsnt_con).squeeze(2)\n",
    "\n",
    "            pred.extend(output[:,-1].cpu().numpy().tolist())\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "HIDDEN_DIM = 128\n",
    "EMB_SIZE = 64\n",
    "N_HEADS = 2\n",
    "N_LAYERS = 1\n",
    "DROPOUT_RATIO = 0.3\n",
    "NUM_WORKERS = 8\n",
    "\n",
    "\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "SEED = 44\n",
    "\n",
    "DATA_PATH = '/opt/ml/ephemeral/data'\n",
    "MODEL_PATH = '/opt/ml/ephemeral/model'\n",
    "SUBMISSION_PATH = '/opt/ml/ephemeral/submission'\n",
    "\n",
    "model_name = 'sasrec+lstm.pt'\n",
    "submission_name = 'sasrec+lstm.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(MODEL_PATH):\n",
    "    os.mkdir(MODEL_PATH)\n",
    "if not os.path.isdir(SUBMISSION_PATH):\n",
    "    os.mkdir(SUBMISSION_PATH)\n",
    "\n",
    "dataset_ = DTset(DATA_PATH=DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['elapsed', 'answerCode_rate']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_.con_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OOF-0| Epoch:   1| Train loss: 0.50521| roc_auc: 0.81158: 100%|██████████| 1/1 [01:36<00:00, 96.22s/it]\n",
      "OOF-0| Epoch:   2| Train loss: 0.46351| roc_auc: 0.82343: 100%|██████████| 1/1 [01:35<00:00, 95.31s/it]\n",
      "OOF-0| Epoch:   3| Train loss: 0.45169| roc_auc: 0.82576: 100%|██████████| 1/1 [01:36<00:00, 96.68s/it]\n",
      "OOF-0| Epoch:   4| Train loss: 0.44557| roc_auc: 0.82865: 100%|██████████| 1/1 [01:37<00:00, 97.28s/it]\n",
      "OOF-0| Epoch:   5| Train loss: 0.44161| roc_auc: 0.83125: 100%|██████████| 1/1 [01:36<00:00, 96.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST OOF-0| Epoch:   5| Train loss: 0.44161| roc_auc: 0.83125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OOF-1| Epoch:   1| Train loss: 0.50379| roc_auc: 0.80562: 100%|██████████| 1/1 [01:38<00:00, 98.21s/it]\n",
      "OOF-1| Epoch:   2| Train loss: 0.46253| roc_auc: 0.81265: 100%|██████████| 1/1 [01:34<00:00, 94.93s/it]\n",
      "OOF-1| Epoch:   3| Train loss: 0.45134| roc_auc: 0.80920: 100%|██████████| 1/1 [01:37<00:00, 97.22s/it]\n",
      "OOF-1| Epoch:   4| Train loss: 0.44564| roc_auc: 0.81807: 100%|██████████| 1/1 [01:35<00:00, 95.64s/it]\n",
      "OOF-1| Epoch:   5| Train loss: 0.44151| roc_auc: 0.81892: 100%|██████████| 1/1 [01:35<00:00, 95.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST OOF-1| Epoch:   5| Train loss: 0.44151| roc_auc: 0.81892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OOF-2| Epoch:   1| Train loss: 0.50446| roc_auc: 0.81048: 100%|██████████| 1/1 [01:37<00:00, 97.13s/it]\n",
      "OOF-2| Epoch:   2| Train loss: 0.46206| roc_auc: 0.81582: 100%|██████████| 1/1 [01:37<00:00, 97.01s/it]\n",
      "OOF-2| Epoch:   3| Train loss: 0.45073| roc_auc: 0.81687: 100%|██████████| 1/1 [01:35<00:00, 95.86s/it]\n",
      "OOF-2| Epoch:   4| Train loss: 0.44535| roc_auc: 0.81663: 100%|██████████| 1/1 [01:37<00:00, 97.39s/it]\n",
      "OOF-2| Epoch:   5| Train loss: 0.44092| roc_auc: 0.82017: 100%|██████████| 1/1 [01:36<00:00, 96.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST OOF-2| Epoch:   5| Train loss: 0.44092| roc_auc: 0.82017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OOF-3| Epoch:   1| Train loss: 0.50473| roc_auc: 0.82930: 100%|██████████| 1/1 [01:36<00:00, 96.38s/it]\n",
      "OOF-3| Epoch:   2| Train loss: 0.46206| roc_auc: 0.84056: 100%|██████████| 1/1 [01:36<00:00, 96.73s/it]\n",
      "OOF-3| Epoch:   3| Train loss: 0.45154| roc_auc: 0.84388: 100%|██████████| 1/1 [01:38<00:00, 98.42s/it]\n",
      "OOF-3| Epoch:   4| Train loss: 0.44518| roc_auc: 0.84624: 100%|██████████| 1/1 [01:36<00:00, 96.27s/it]\n",
      "OOF-3| Epoch:   5| Train loss: 0.44187| roc_auc: 0.84367: 100%|██████████| 1/1 [01:36<00:00, 96.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST OOF-3| Epoch:   4| Train loss: 0.44518| roc_auc: 0.84624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OOF-4| Epoch:   1| Train loss: 0.50587| roc_auc: 0.81909: 100%|██████████| 1/1 [01:36<00:00, 96.96s/it]\n",
      "OOF-4| Epoch:   2| Train loss: 0.46253| roc_auc: 0.82452: 100%|██████████| 1/1 [01:37<00:00, 97.43s/it]\n",
      "OOF-4| Epoch:   3| Train loss: 0.45089| roc_auc: 0.82875: 100%|██████████| 1/1 [01:37<00:00, 97.71s/it]\n",
      "OOF-4| Epoch:   4| Train loss: 0.44531| roc_auc: 0.83279: 100%|██████████| 1/1 [01:36<00:00, 96.57s/it]\n",
      "OOF-4| Epoch:   5| Train loss: 0.44134| roc_auc: 0.83214: 100%|██████████| 1/1 [01:37<00:00, 97.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST OOF-4| Epoch:   4| Train loss: 0.44531| roc_auc: 0.83279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DTset' object has no attribute 'oof_user_set'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-4f7cc5e3861f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0moof_roc_auc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbest_roc_auc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Total roc_auc: {oof_roc_auc / len(dataset_.oof_user_set.keys()):.5f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DTset' object has no attribute 'oof_user_set'"
     ]
    }
   ],
   "source": [
    "oof_roc_auc = 0\n",
    "\n",
    "for oof in dataset_.oof_ID_set.keys():\n",
    "    train_df,valid_df = dataset_.get_oof(oof)\n",
    "\n",
    "    seed_everything(4444+ oof)\n",
    "\n",
    "    train_dataset = Custom(df = train_df,)\n",
    "    train_data_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "        collate_fn = train_make_batch,\n",
    "        num_workers=NUM_WORKERS)\n",
    "    \n",
    "    valid_dataset = Custom(df = valid_df,)\n",
    "    valid_data_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size = 1,\n",
    "        shuffle = False, \n",
    "        drop_last = False,\n",
    "        collate_fn = train_make_batch,\n",
    "        num_workers = NUM_WORKERS)\n",
    "\n",
    "    model = sasreclstm(\n",
    "        n_assID=dataset_.n_assItemID_,\n",
    "        n_testID=dataset_.n_testId_,\n",
    "        n_tag=dataset_.n_tag_,\n",
    "        n_level=dataset_.n_level_,\n",
    "        n_dayweek=dataset_.n_dayweek,\n",
    "        con_cols=dataset_.con_cols,\n",
    "        cat_cols=dataset_.cat_cols,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        emb_size=EMB_SIZE,\n",
    "        n_heads=N_HEADS,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout_ratio=DROPOUT_RATIO,\n",
    "        device = device\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    best_epoch = 0\n",
    "    best_train_loss = 0\n",
    "    best_roc_auc = 0\n",
    "\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        tbar = tqdm(range(1))\n",
    "        for _ in tbar:\n",
    "            train_loss = train(model = model, data_loader = train_data_loader, criterion = criterion, optimizer = optimizer)\n",
    "            roc_auc = evaluate(model = model, data_loader = valid_data_loader)\n",
    "            if best_roc_auc < roc_auc:\n",
    "                best_epoch = epoch\n",
    "                best_train_loss = train_loss\n",
    "                best_roc_auc = roc_auc\n",
    "                torch.save(model.state_dict(), os.path.join(MODEL_PATH, f'oof_{oof}_' + model_name))\n",
    "\n",
    "            tbar.set_description(f'OOF-{oof}| Epoch: {epoch:3d}| Train loss: {train_loss:.5f}| roc_auc: {roc_auc:.5f}')\n",
    "    \n",
    "    print(f'BEST OOF-{oof}| Epoch: {best_epoch:3d}| Train loss: {best_train_loss:.5f}| roc_auc: {best_roc_auc:.5f}')\n",
    "\n",
    "    oof_roc_auc += best_roc_auc\n",
    "\n",
    "print(f'Total roc_auc: {oof_roc_auc / len(dataset_.oof_ID_set.keys()):.5f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = dataset_.get_test_data()\n",
    "test_dataset = Custom(df = test_df,)\n",
    "test_data_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size = 1, \n",
    "    shuffle = False, \n",
    "    drop_last = False,\n",
    "    collate_fn = train_make_batch,\n",
    "    num_workers = NUM_WORKERS)\n",
    "\n",
    "pred_list = []\n",
    "\n",
    "model = sasreclstm(\n",
    "    n_assID=dataset_.n_assItemID_,\n",
    "    n_testID=dataset_.n_testId_,\n",
    "    n_tag=dataset_.n_tag_,\n",
    "    n_level=dataset_.n_level_,\n",
    "    n_dayweek=dataset_.n_dayweek,\n",
    "    con_cols=dataset_.con_cols,\n",
    "    cat_cols=dataset_.cat_cols,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    emb_size=EMB_SIZE,\n",
    "    n_heads=N_HEADS,\n",
    "    n_layers=N_LAYERS,\n",
    "    dropout_ratio=DROPOUT_RATIO,\n",
    "    device = device\n",
    ").to(device)\n",
    "\n",
    "for oof in dataset_.oof_ID_set.keys():\n",
    "    model.load_state_dict(torch.load(os.path.join(MODEL_PATH, f'oof_{oof}_' + model_name)))\n",
    "    pred = predict(model = model, data_loader = test_data_loader)\n",
    "    pred_list.append(pred)\n",
    "\n",
    "pred_list = np.array(pred_list).mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.44461695, 0.42473261, 0.21767459, 0.58092774, 0.20079038,\n",
       "       0.88046427, 0.02589853, 0.09366444, 0.22762547, 0.86788585,\n",
       "       0.09784036, 0.43001171, 0.76825461, 0.14653318, 0.54860221,\n",
       "       0.94305134, 0.17911665, 0.91253386, 0.8237928 , 0.1658613 ,\n",
       "       0.85763954, 0.59254133, 0.39833657, 0.21274736, 0.21589939,\n",
       "       0.6050535 , 0.60782969, 0.89318601, 0.34233879, 0.67827566,\n",
       "       0.5349436 , 0.71009325, 0.1657834 , 0.01055232, 0.64424632,\n",
       "       0.69192712, 0.31876088, 0.46021404, 0.09738726, 0.08392041,\n",
       "       0.18480654, 0.09315975, 0.16194888, 0.38218744, 0.705803  ,\n",
       "       0.54373105, 0.63042635, 0.19969183, 0.5554188 , 0.74759204,\n",
       "       0.63201053, 0.27047498, 0.21028913, 0.10861754, 0.28120146,\n",
       "       0.59018484, 0.41380625, 0.94425128, 0.10056709, 0.04735683,\n",
       "       0.86916388, 0.88889718, 0.70595224, 0.26870397, 0.15333382,\n",
       "       0.29540511, 0.66309729, 0.15116958, 0.19339506, 0.36213521,\n",
       "       0.66631324, 0.67494951, 0.05834267, 0.18968308, 0.61567356,\n",
       "       0.60746099, 0.17170673, 0.62496763, 0.10823404, 0.81176616,\n",
       "       0.28156827, 0.72198163, 0.68769131, 0.69224137, 0.73133585,\n",
       "       0.12797821, 0.48251154, 0.01406118, 0.49116503, 0.2095946 ,\n",
       "       0.20871332, 0.05510789, 0.19008297, 0.65828419, 0.88738955,\n",
       "       0.49284912, 0.14954075, 0.87172239, 0.88840702, 0.93285891,\n",
       "       0.28040784, 0.21839155, 0.61453139, 0.09968598, 0.1801888 ,\n",
       "       0.87863175, 0.14262889, 0.91857616, 0.02947365, 0.48308975,\n",
       "       0.21102036, 0.94448547, 0.11629175, 0.06742808, 0.1902289 ,\n",
       "       0.87318368, 0.72406325, 0.20808742, 0.28948717, 0.24220719,\n",
       "       0.50734571, 0.92723475, 0.43050305, 0.10048518, 0.0061362 ,\n",
       "       0.06166371, 0.79183537, 0.06757026, 0.83245665, 0.04306785,\n",
       "       0.92311203, 0.16469834, 0.78567404, 0.25340658, 0.59462148,\n",
       "       0.48398279, 0.11838912, 0.69115561, 0.21735572, 0.70774089,\n",
       "       0.73229673, 0.5391    , 0.1830645 , 0.49564077, 0.62138592,\n",
       "       0.08860757, 0.92410501, 0.9160689 , 0.07140295, 0.46560932,\n",
       "       0.05374441, 0.5262167 , 0.7724068 , 0.00899047, 0.19294775,\n",
       "       0.48737419, 0.08521075, 0.3067711 , 0.71242621, 0.08850728,\n",
       "       0.40970887, 0.11725681, 0.48231146, 0.09780006, 0.56211848,\n",
       "       0.16669214, 0.02852647, 0.11930526, 0.68548712, 0.39507293,\n",
       "       0.80271003, 0.14127085, 0.26751878, 0.02912226, 0.2384572 ,\n",
       "       0.845002  , 0.80249345, 0.04650309, 0.08104867, 0.18074315,\n",
       "       0.15350178, 0.0909499 , 0.5055142 , 0.10262258, 0.90330836,\n",
       "       0.4744931 , 0.39096775, 0.4153768 , 0.92462418, 0.14296525,\n",
       "       0.54452285, 0.21015299, 0.39860017, 0.71283221, 0.57147561,\n",
       "       0.63464911, 0.9141428 , 0.76185082, 0.83912761, 0.06510668,\n",
       "       0.50810925, 0.46095207, 0.17414837, 0.69996918, 0.16896447,\n",
       "       0.79895554, 0.01217986, 0.06219942, 0.18256975, 0.01599215,\n",
       "       0.04105879, 0.58764645, 0.26051348, 0.2114206 , 0.2255702 ,\n",
       "       0.31090535, 0.70520902, 0.58686458, 0.87260604, 0.64817365,\n",
       "       0.51787744, 0.60798053, 0.04214244, 0.91118672, 0.90948105,\n",
       "       0.16742065, 0.60933567, 0.10895687, 0.23103866, 0.12205556,\n",
       "       0.08940778, 0.66272845, 0.53668336, 0.08693992, 0.69008516,\n",
       "       0.75806541, 0.30622104, 0.0965463 , 0.19994486, 0.69851135,\n",
       "       0.68490795, 0.2256999 , 0.0976971 , 0.82127339, 0.94035453,\n",
       "       0.46218012, 0.11187195, 0.51900076, 0.12895975, 0.08329536,\n",
       "       0.93843969, 0.80048174, 0.49531848, 0.68397075, 0.45788672,\n",
       "       0.08323839, 0.69825197, 0.68997355, 0.07601056, 0.38125003,\n",
       "       0.02578447, 0.75235835, 0.02763386, 0.14581969, 0.17521545,\n",
       "       0.12553773, 0.87911491, 0.8608716 , 0.08426892, 0.00619675,\n",
       "       0.06776637, 0.1661552 , 0.27806213, 0.72022319, 0.65837333,\n",
       "       0.1216662 , 0.52211809, 0.00728501, 0.28295735, 0.4929451 ,\n",
       "       0.63245952, 0.32565896, 0.70915706, 0.93106914, 0.52220486,\n",
       "       0.0595375 , 0.10572005, 0.1443909 , 0.59622576, 0.72891201,\n",
       "       0.43557754, 0.12598917, 0.06188767, 0.01467973, 0.41571226,\n",
       "       0.27315462, 0.41774544, 0.79912642, 0.40640718, 0.54294176,\n",
       "       0.46323968, 0.11544571, 0.13937464, 0.76010684, 0.49707869,\n",
       "       0.06175211, 0.52611403, 0.89586431, 0.10390414, 0.34858284,\n",
       "       0.10696907, 0.01329424, 0.70726637, 0.43654078, 0.36535045,\n",
       "       0.84205498, 0.8716278 , 0.37891748, 0.24861811, 0.35704571,\n",
       "       0.1596764 , 0.33807148, 0.65771556, 0.27080913, 0.55281253,\n",
       "       0.04447618, 0.73776975, 0.07169795, 0.7760671 , 0.41766713,\n",
       "       0.93660547, 0.07296291, 0.20463179, 0.1557181 , 0.83582441,\n",
       "       0.52102574, 0.09006014, 0.51136469, 0.82418034, 0.78072577,\n",
       "       0.73560526, 0.53865718, 0.58737787, 0.07064247, 0.58897249,\n",
       "       0.93356503, 0.48231441, 0.06238258, 0.51898688, 0.63715907,\n",
       "       0.43898976, 0.5049257 , 0.34737118, 0.70953764, 0.77561339,\n",
       "       0.59731978, 0.71134961, 0.13404737, 0.04793813, 0.35314985,\n",
       "       0.39791236, 0.06162996, 0.06000376, 0.20888156, 0.06941499,\n",
       "       0.42834016, 0.41949131, 0.11094878, 0.12671641, 0.0754506 ,\n",
       "       0.57381269, 0.66175274, 0.78967932, 0.85566648, 0.16390658,\n",
       "       0.76703004, 0.74770235, 0.32907439, 0.67196598, 0.36525639,\n",
       "       0.06632117, 0.96892375, 0.57644768, 0.12649848, 0.27908007,\n",
       "       0.23917188, 0.38480128, 0.76897224, 0.86167152, 0.36427442,\n",
       "       0.28872313, 0.1188615 , 0.14093948, 0.18352028, 0.81619232,\n",
       "       0.05222569, 0.60580317, 0.71090674, 0.10820312, 0.10925764,\n",
       "       0.85329726, 0.84431577, 0.08891211, 0.07418913, 0.06715221,\n",
       "       0.55169466, 0.84800107, 0.13331931, 0.13878974, 0.14359687,\n",
       "       0.49275764, 0.04273424, 0.18251176, 0.33743701, 0.08764518,\n",
       "       0.4361616 , 0.75647342, 0.74009805, 0.64799883, 0.63188764,\n",
       "       0.77464147, 0.46890481, 0.22704071, 0.57562714, 0.8268609 ,\n",
       "       0.08823518, 0.76661717, 0.17254305, 0.59082917, 0.47323696,\n",
       "       0.62455875, 0.12390651, 0.72901969, 0.68449973, 0.46617926,\n",
       "       0.61692203, 0.13832778, 0.14554903, 0.78229579, 0.35914611,\n",
       "       0.11620082, 0.18922814, 0.07631285, 0.82342328, 0.04724321,\n",
       "       0.84234763, 0.15113919, 0.8787416 , 0.29349369, 0.29389362,\n",
       "       0.45532816, 0.75688102, 0.34716141, 0.31677408, 0.07023198,\n",
       "       0.41190363, 0.17973419, 0.59722334, 0.49956738, 0.07235269,\n",
       "       0.42578257, 0.86667508, 0.27520889, 0.39762052, 0.43335292,\n",
       "       0.67895448, 0.848078  , 0.0081362 , 0.36088232, 0.15142219,\n",
       "       0.02260913, 0.042114  , 0.65764942, 0.59636039, 0.29509315,\n",
       "       0.33308313, 0.36032041, 0.0900913 , 0.67509288, 0.649638  ,\n",
       "       0.6702245 , 0.46474817, 0.08783157, 0.05321279, 0.8804689 ,\n",
       "       0.07173511, 0.25622837, 0.0464921 , 0.10515705, 0.04960314,\n",
       "       0.08953652, 0.12963239, 0.29038771, 0.06966999, 0.48685132,\n",
       "       0.01704299, 0.19514425, 0.40048127, 0.20780209, 0.62097907,\n",
       "       0.55566721, 0.54820275, 0.22450967, 0.13563   , 0.62108619,\n",
       "       0.25488856, 0.69334966, 0.81352241, 0.43961526, 0.1129846 ,\n",
       "       0.28988625, 0.08841944, 0.74861546, 0.65611869, 0.64386322,\n",
       "       0.46731442, 0.85870426, 0.09999802, 0.39287527, 0.27824599,\n",
       "       0.81781113, 0.70083321, 0.84964263, 0.06714634, 0.81826363,\n",
       "       0.07567054, 0.65930797, 0.23236677, 0.83155868, 0.64436529,\n",
       "       0.6417215 , 0.08103601, 0.14075295, 0.37229525, 0.11434281,\n",
       "       0.54602453, 0.65608282, 0.57572905, 0.01270568, 0.26393332,\n",
       "       0.95834163, 0.62743182, 0.72822645, 0.35832369, 0.32295258,\n",
       "       0.84954313, 0.89469771, 0.80292412, 0.81308624, 0.79698974,\n",
       "       0.48525432, 0.47946267, 0.15439793, 0.53341252, 0.26319187,\n",
       "       0.52068598, 0.62976054, 0.4873328 , 0.58691387, 0.42849777,\n",
       "       0.6170119 , 0.75824205, 0.4554379 , 0.08412496, 0.13433463,\n",
       "       0.67627754, 0.0445874 , 0.83766557, 0.05266749, 0.21017742,\n",
       "       0.12715369, 0.3000735 , 0.0178894 , 0.1665854 , 0.4073065 ,\n",
       "       0.44655419, 0.05138893, 0.43542847, 0.76064383, 0.5953825 ,\n",
       "       0.48752068, 0.78930022, 0.8902077 , 0.01800936, 0.11957215,\n",
       "       0.22164141, 0.58053569, 0.30497543, 0.45135592, 0.2550087 ,\n",
       "       0.15452973, 0.46244341, 0.80516316, 0.41759659, 0.33770402,\n",
       "       0.34761837, 0.92195077, 0.61379544, 0.03165594, 0.60485457,\n",
       "       0.82113941, 0.02432108, 0.64195516, 0.19938516, 0.80473617,\n",
       "       0.68031363, 0.32220867, 0.61338189, 0.3105484 , 0.85047886,\n",
       "       0.497453  , 0.14003855, 0.89032897, 0.01645252, 0.42448401,\n",
       "       0.66688135, 0.7792789 , 0.05392875, 0.18401357, 0.01060472,\n",
       "       0.07162397, 0.05902962, 0.09052718, 0.18842146, 0.17945115,\n",
       "       0.03943466, 0.42326397, 0.78952817, 0.07057426, 0.17677944,\n",
       "       0.10186778, 0.56364733, 0.43675293, 0.45860949, 0.56801562,\n",
       "       0.02124984, 0.21562262, 0.30085395, 0.49424762, 0.21069863,\n",
       "       0.72135384, 0.67388921, 0.01234192, 0.35552746, 0.13875872,\n",
       "       0.12971287, 0.12136696, 0.8094861 , 0.15958478, 0.2748834 ,\n",
       "       0.03022044, 0.65440016, 0.83182389, 0.65314146, 0.04081659,\n",
       "       0.34290941, 0.46471339, 0.79008267, 0.13770116, 0.17311275,\n",
       "       0.29966214, 0.76710385, 0.51545299, 0.11282737, 0.70538177,\n",
       "       0.44782301, 0.55411193, 0.21082783, 0.52802556, 0.56786865,\n",
       "       0.22125094, 0.06777456, 0.80848318, 0.11746643, 0.09294888,\n",
       "       0.86714416, 0.00799879, 0.73448055, 0.18406202, 0.11268829,\n",
       "       0.13457635, 0.34620695, 0.47377908, 0.87390724, 0.59016483,\n",
       "       0.5389194 , 0.65139862, 0.70402979, 0.87526727, 0.5737625 ,\n",
       "       0.41440642, 0.71247137, 0.60571828, 0.49044651, 0.44491311,\n",
       "       0.01883593, 0.06807627, 0.0464154 , 0.13514223, 0.77753071,\n",
       "       0.42647846, 0.84999048, 0.76938585, 0.6109239 , 0.0407599 ,\n",
       "       0.06744525, 0.01643778, 0.52952961, 0.02706146, 0.05060496,\n",
       "       0.89451303, 0.23625989, 0.34417041, 0.36002141, 0.00780031,\n",
       "       0.36646248, 0.77026459, 0.26204703, 0.50306962, 0.24345233,\n",
       "       0.26207021, 0.40941563, 0.04992938, 0.86557353, 0.67646563,\n",
       "       0.61755998, 0.08208207, 0.05019028, 0.6046751 , 0.02108161,\n",
       "       0.85237544, 0.25994665, 0.34500924, 0.56585552, 0.2726438 ,\n",
       "       0.6367043 , 0.81328614, 0.22622065, 0.60434122, 0.0054746 ,\n",
       "       0.86114753, 0.78797538, 0.59174069, 0.68791931])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(data = np.array(pred_list), columns = ['prediction'])\n",
    "submission['id'] = submission.index\n",
    "submission = submission[['id', 'prediction']]\n",
    "submission.to_csv(os.path.join(SUBMISSION_PATH, 'OOF-Ensemble-' + submission_name), index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dkt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
