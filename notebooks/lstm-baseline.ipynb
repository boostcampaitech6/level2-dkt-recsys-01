{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple LSTM Baseline\n",
    "\n",
    "- Transformer 적용 이전, 성능의 원활한 비교를 위해 DNN baseline으로 LSTM 모델을 빌드함\n",
    "- 간단한 구조로 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import wandb\n",
    "\n",
    "plt.rcParams[\"font.family\"] = 'NanumGothic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "_ = torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prep dataset, dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "\n",
    "train_df = pd.read_csv('../../data/train_data.csv')\n",
    "test_df = pd.read_csv('../../data/test_data.csv')\n",
    "\n",
    "class MultiLabelEncoder:\n",
    "    # 참고: https://stackoverflow.com/questions/24458645/label-encoding-across-multiple-columns-in-scikit-learn\n",
    "\n",
    "    def __init__(self, df):\n",
    "        # define encoder dict\n",
    "        self.d = defaultdict(LabelEncoder)\n",
    "        # Encoding the variable\n",
    "        for col in df.columns:\n",
    "            self.d[col] = self.d[col].fit(df[col])\n",
    "    \n",
    "    def encode(self, df):\n",
    "        return df.apply(lambda x: self.d[x.name].transform(x))\n",
    "\n",
    "    def decode(self, df):\n",
    "    # Inverse the encoded\n",
    "        return df.apply(lambda x: self.d[x.name].inverse_transform(x))\n",
    "\n",
    "category_cols = ['assessmentItemID', 'testId', 'KnowledgeTag']\n",
    "mle = MultiLabelEncoder(train_df[category_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get sequences by user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_by_user(df, features, max_length=512, train=True):\n",
    "    user_ids, inputs, masks, targets = [], [], [], []\n",
    "\n",
    "    for user_id in tqdm(df['userID'].unique()):\n",
    "\n",
    "        # get user data with user_id\n",
    "        user_data = df[df['userID'] == user_id]\n",
    "        # get additional info (previous label)\n",
    "        user_data = user_data.assign(previous_label=(user_data.answerCode.shift(1) + 1).fillna(0).values)\n",
    "        # get sequence to numpy\n",
    "        sequence = user_data[features].to_numpy()\n",
    "        # get target data: last answerCode\n",
    "        target = user_data['answerCode'].values[-1]\n",
    "\n",
    "        # cut or pad sequences with max_length\n",
    "        if len(sequence) < max_length:\n",
    "            padding = np.zeros((max_length - len(sequence), sequence.shape[1]))\n",
    "            mask = np.vstack((padding, np.ones_like(sequence)))\n",
    "            sequence = np.vstack((padding, sequence))\n",
    "        else:\n",
    "            sequence = sequence[-max_length:]\n",
    "            mask = np.ones((max_length, sequence.shape[1]))\n",
    "        \n",
    "        user_ids.append(user_id)\n",
    "        inputs.append(sequence)\n",
    "        masks.append(mask)\n",
    "        targets.append(target)\n",
    "\n",
    "    return np.array(user_ids), np.array(inputs), np.array(masks), np.array(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_path, mle, category_cols, features, max_length, train=True):\n",
    "    # read data\n",
    "    df = pd.read_csv(data_path)\n",
    "    # preprocess\n",
    "    # label encoding\n",
    "    df_encoded = mle.encode(df[category_cols])\n",
    "    df = pd.concat([df.drop(category_cols, axis=1), df_encoded], axis=1)\n",
    "\n",
    "    # sequence\n",
    "    return get_sequence_by_user(df, features, max_length, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6698/6698 [00:31<00:00, 215.92it/s]\n",
      "100%|██████████| 744/744 [00:01<00:00, 415.38it/s]\n"
     ]
    }
   ],
   "source": [
    "train_path = '../../data/train_data.csv'\n",
    "test_path = '../../data/test_data.csv'\n",
    "features = ['assessmentItemID','testId','KnowledgeTag', 'previous_label']\n",
    "max_length = 512\n",
    "\n",
    "train_data = get_data(train_path, mle, category_cols, features, max_length, train=True)\n",
    "test_data = get_data(test_path, mle, category_cols, features, max_length, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_val_split(user_ids, X, masks, target, train_size=.8):\n",
    "    # split index and target\n",
    "    train_index, valid_index, train_y, valid_y = train_test_split(\n",
    "        range(target.shape[0]), target, train_size=.8, stratify=target)\n",
    "    # split X\n",
    "    train_X, valid_X = X[train_index], X[valid_index]\n",
    "    # split masks\n",
    "    train_masks, valid_masks = masks[train_index], masks[valid_index]\n",
    "    # split users\n",
    "    train_users, valid_users = user_ids[train_index], user_ids[valid_index]\n",
    "\n",
    "    return (train_users, train_X, train_masks, train_y), (valid_users, valid_X, valid_masks, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = train_val_split(*train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prep dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSequenceDKTDataset(Dataset):\n",
    "    def __init__(self, user_ids, X, mask, y=None, max_length=512, train=True):\n",
    "        super().__init__()\n",
    "        self.train = train\n",
    "\n",
    "        self.user_ids = user_ids\n",
    "        self.X = X\n",
    "        self.mask = mask\n",
    "        if self.train:\n",
    "            self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.user_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item = {'user_id': self.user_ids[index]}\n",
    "        item['X'] = self.X[index]\n",
    "        item['mask'] = self.mask[index]\n",
    "        if self.train:\n",
    "            item['y'] = self.y[index]\n",
    "        return item\n",
    "    \n",
    "    def get_user_ids(self):\n",
    "        return self.user_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5358, 1340, 744)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = SimpleSequenceDKTDataset(*train_data)\n",
    "valid_dataset = SimpleSequenceDKTDataset(*valid_data)\n",
    "test_dataset = SimpleSequenceDKTDataset(*test_data, train=False)\n",
    "len(train_dataset), len(valid_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'modelname': 'lstm', 'result_dir': '../results/', 'data_dir': '.', 'data_version': 'v1', 'wandb_config': {'wandb': True, 'project_name': 'dkt-dl'}, 'train_config': {'batch_size': 128, 'emb_dim': 16, 'hidden_dim': 16, 'activation_f': 'tanh', 'learnin_rate': 0.1, 'epochs': 100, 'patience': 20, 'T_max': 10, 'eta_min': 0}}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "#read yaml file\n",
    "with open('rnn_config.yaml') as file:\n",
    "  config = yaml.safe_load(file)  \n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wandb logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtwndus\u001b[0m (\u001b[33mdkt-recsys1\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/level2-dkt-recsys-01/notebooks/wandb/run-20240111_070508-p3g6zvjq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dkt-recsys1/dkt-dl/runs/p3g6zvjq' target=\"_blank\">lstm_240111-070506</a></strong> to <a href='https://wandb.ai/dkt-recsys1/dkt-dl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dkt-recsys1/dkt-dl' target=\"_blank\">https://wandb.ai/dkt-recsys1/dkt-dl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dkt-recsys1/dkt-dl/runs/p3g6zvjq' target=\"_blank\">https://wandb.ai/dkt-recsys1/dkt-dl/runs/p3g6zvjq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# run name condition\n",
    "now = dt.strftime(dt.now(), '%y%m%d-%H%M%S')\n",
    "\n",
    "if config['wandb_config']['wandb']:\n",
    "    run = wandb.init(\n",
    "        project=config['wandb_config']['project_name'],\n",
    "        name=f'{config[\"modelname\"]}_{now}',\n",
    "        config={\n",
    "        'dataset_version': config['data_version'],\n",
    "        **config['train_config'],\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = config['data_dir']\n",
    "data_version = config['data_version']\n",
    "\n",
    "train_dataset_v1 = torch.load(os.path.join(data_dir, f'train_dataset_{data_version}.pt'))\n",
    "valid_dataset_v1 = torch.load(os.path.join(data_dir, f'valid_dataset_{data_version}.pt'))\n",
    "test_dataset_v1 = torch.load(os.path.join(data_dir, f'test_dataset_{data_version}.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = config['train_config']['batch_size']\n",
    "train_dataloader = DataLoader(train_dataset_v1, batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset_v1, batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset_v1, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_size = len(features[:-1])\n",
    "cat_emb_size = [len(mle.d[feature].classes_) for feature in features[:-1]]\n",
    "num_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTMModel(nn.Module):\n",
    "\n",
    "    def __init__(self, features, cat_size, num_size, cat_emb_size, \n",
    "        emb_dim, hidden_dim, activation_f):\n",
    "        super().__init__()\n",
    "        # for categorical data\n",
    "        self.embedding = nn.Embedding(sum(cat_emb_size)+cat_size, emb_dim, dtype=torch.float32)\n",
    "        self.cat_linear = nn.Linear(emb_dim*cat_size, hidden_dim//2, dtype=torch.float32)\n",
    "        self.cat_layernorm = nn.LayerNorm(hidden_dim//2)\n",
    "        # for continuous data\n",
    "        self.cont_linear = nn.Linear(num_size, hidden_dim//2, dtype=torch.float32)\n",
    "        self.cont_layernorm = nn.LayerNorm(hidden_dim//2)\n",
    "        # lstm cell\n",
    "        self.lstm_cell = nn.LSTMCell(hidden_dim, hidden_dim, activation_f, dtype=torch.float32)\n",
    "        self.last_layer = nn.Linear(hidden_dim, 1, dtype=torch.float32)\n",
    "\n",
    "    def init_params(self):\n",
    "        # lstm\n",
    "        nn.init.kaiming_uniform_(self.lstm_cell.weight_ih)\n",
    "        nn.init.kaiming_uniform_(self.lstm_cell.weight_hh)\n",
    "        nn.init.zeros_(self.lstm_cell.bias_ih)\n",
    "        nn.init.zeros_(self.lstm_cell.bias_hh)\n",
    "        # last layer\n",
    "        nn.init.kaiming_uniform_(self.last_layer.weights)\n",
    "        nn.init.zeros_(self.last_layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, mask = x\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        # categorical data embedding\n",
    "        offset = x.new_tensor(np.array([0, *np.cumsum(cat_emb_size)[:-1]])) + 1\n",
    "        x_cat = (x[:,:,:-1] + offset).mul(mask[:,:,:-1])\n",
    "        x_cat = self.embedding(x_cat.int()).view(batch_size, seq_len, -1)\n",
    "        x_cat = self.cat_linear(x_cat)\n",
    "        x_cat = self.cat_layernorm(x_cat)\n",
    "\n",
    "        # continuous\n",
    "        x_cont = x[:,:,-1].view(batch_size, seq_len, -1)\n",
    "        x_cont = self.cont_linear(x_cont)\n",
    "        x_cont = self.cont_layernorm(x_cont)\n",
    "\n",
    "        # concat data\n",
    "        x_concat = torch.concat([x_cat, x_cont], dim=-1)\n",
    "\n",
    "        # Initial hidden state and cell state\n",
    "        lstm_h0 = torch.rand(batch_size, self.lstm_cell.hidden_size).to(device)\n",
    "        lstm_c0 = torch.rand(batch_size, self.lstm_cell.hidden_size).to(device)\n",
    "        \n",
    "        for e in range(seq_len):\n",
    "            lstm_h0, lstm_c0 = self.lstm_cell(x_concat[:, e, :], (lstm_h0, lstm_c0))\n",
    "\n",
    "        output = self.last_layer(lstm_h0)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = config['train_config']['learnin_rate']\n",
    "T_max = config['train_config']['T_max']\n",
    "eta_min = config['train_config']['eta_min']\n",
    "\n",
    "seqlen = 512 #DATA\n",
    "input_feature = 4 #DATA\n",
    "emb_dim = config['train_config']['emb_dim']\n",
    "hidden_dim = config['train_config']['hidden_dim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # at beginning of the script\n",
    "# device = torch.device('cpu')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SimpleLSTMModel(features, cat_size, num_size, cat_emb_size, \n",
    "    emb_dim, hidden_dim, 'tanh').to(device)\n",
    "\n",
    "loss_f = nn.BCEWithLogitsLoss().to(device)\n",
    "adamw = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "lr_schedular = torch.optim.lr_scheduler.CosineAnnealingLR(adamw, T_max=T_max, eta_min=eta_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train_step(model, loss_f, train_dataloader, adamw, scheduler, device):\n",
    "    loss_sum, auc_sum = 0, 0\n",
    "\n",
    "    for iter, data in enumerate(train_dataloader):\n",
    "\n",
    "        X, y = data['X'].float().to(device), data['y'].view(data['y'].size(0), 1).float().to(device)\n",
    "        mask = data['mask'].float().to(device)\n",
    "    \n",
    "        pred = model((X, mask))\n",
    "        loss = loss_f(pred, y)\n",
    "        \n",
    "        adamw.zero_grad()\n",
    "        loss.backward()\n",
    "        adamw.step()\n",
    "\n",
    "        auc_sum += roc_auc_score(y.detach().cpu().numpy(), (torch.sigmoid(pred).detach().cpu().numpy()>=0.5).astype(int))\n",
    "        loss_sum += loss.item()\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    return loss_sum/len(train_dataloader), auc_sum/len(train_dataloader)\n",
    "\n",
    "def valid_step(model, loss_f, valid_dataloader, device):\n",
    "    loss_sum = 0\n",
    "    auc_sum = 0\n",
    "    targets, preds = [], []\n",
    "\n",
    "    for iter, data in enumerate(valid_dataloader):\n",
    "        X, y = data['X'].float().to(device), data['y'].view(data['y'].size(0), 1).float().to(device)\n",
    "        mask = data['mask'].float().to(device)\n",
    "        pred = model((X, mask))\n",
    "        \n",
    "        loss = loss_f(pred, y)\n",
    "\n",
    "        auc_sum += roc_auc_score(y.detach().cpu().numpy(), (torch.sigmoid(pred).detach().cpu().numpy()>=0.5).astype(int))\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "        targets.extend(data['y'].detach().numpy())\n",
    "        preds.extend(torch.sigmoid(pred).detach().cpu().numpy())\n",
    "        \n",
    "    return targets, preds, loss_sum/len(valid_dataloader), auc_sum/len(valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 epochs] train_loss: 0.6585, valid_loss: 0.5864, train_auc: 0.5919, valid_auc: 0.6857\n",
      "[1 epochs] train_loss: 0.5779, valid_loss: 0.5815, train_auc: 0.7057, valid_auc: 0.7105\n",
      "[2 epochs] train_loss: 0.5333, valid_loss: 0.6041, train_auc: 0.7422, valid_auc: 0.6721\n",
      "[3 epochs] train_loss: 0.5141, valid_loss: 0.7061, train_auc: 0.7481, valid_auc: 0.6662\n",
      "[4 epochs] train_loss: 0.4902, valid_loss: 0.6335, train_auc: 0.7687, valid_auc: 0.6939\n",
      "[5 epochs] train_loss: 0.4481, valid_loss: 0.6416, train_auc: 0.7910, valid_auc: 0.7025\n",
      "[6 epochs] train_loss: 0.4146, valid_loss: 0.6660, train_auc: 0.8097, valid_auc: 0.6901\n",
      "[7 epochs] train_loss: 0.3903, valid_loss: 0.6856, train_auc: 0.8218, valid_auc: 0.6901\n",
      "[8 epochs] train_loss: 0.3723, valid_loss: 0.6908, train_auc: 0.8334, valid_auc: 0.6888\n",
      "[9 epochs] train_loss: 0.3617, valid_loss: 0.7086, train_auc: 0.8380, valid_auc: 0.6890\n",
      "[10 epochs] train_loss: 0.3583, valid_loss: 0.6969, train_auc: 0.8402, valid_auc: 0.6871\n",
      "[11 epochs] train_loss: 0.3590, valid_loss: 0.7096, train_auc: 0.8407, valid_auc: 0.6871\n",
      "[12 epochs] train_loss: 0.3593, valid_loss: 0.7142, train_auc: 0.8407, valid_auc: 0.6865\n",
      "[13 epochs] train_loss: 0.3614, valid_loss: 0.7272, train_auc: 0.8368, valid_auc: 0.6868\n",
      "[14 epochs] train_loss: 0.3642, valid_loss: 0.7244, train_auc: 0.8343, valid_auc: 0.6885\n",
      "[15 epochs] train_loss: 0.3738, valid_loss: 0.7188, train_auc: 0.8301, valid_auc: 0.6730\n",
      "[16 epochs] train_loss: 0.3962, valid_loss: 0.6729, train_auc: 0.8151, valid_auc: 0.6842\n",
      "[17 epochs] train_loss: 0.4113, valid_loss: 0.7135, train_auc: 0.8131, valid_auc: 0.6822\n",
      "[18 epochs] train_loss: 0.4093, valid_loss: 0.6940, train_auc: 0.8099, valid_auc: 0.6870\n",
      "[19 epochs] train_loss: 0.4169, valid_loss: 0.7385, train_auc: 0.8065, valid_auc: 0.6842\n",
      "[20 epochs] train_loss: 0.4352, valid_loss: 0.6491, train_auc: 0.7939, valid_auc: 0.6825\n",
      "[21 epochs] train_loss: 0.4442, valid_loss: 0.6739, train_auc: 0.7873, valid_auc: 0.6961\n",
      "early stopped at 21 epoch\n"
     ]
    }
   ],
   "source": [
    "epochs = config['train_config']['epochs']\n",
    "best_auc, best_epochs = 0, 0\n",
    "least_loss, patience, num = 1e+9, config['train_config']['patience'], 0\n",
    "best_model = None\n",
    "\n",
    "run.watch(model, log_freq=100)\n",
    "\n",
    "for e in range(epochs):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss, train_auc = train_step(model, loss_f, train_dataloader, adamw, lr_schedular, device)\n",
    "    model.eval()\n",
    "    _, _, valid_loss, valid_auc = valid_step(model, loss_f, valid_dataloader, device)\n",
    "    \n",
    "    if best_auc < valid_auc:\n",
    "        best_auc, best_epochs = valid_auc, e\n",
    "        best_model = deepcopy(model.state_dict())\n",
    "\n",
    "    print(f'[{e} epochs] train_loss: {train_loss:.4f}, valid_loss: {valid_loss:.4f}, train_auc: {train_auc:.4f}, valid_auc: {valid_auc:.4f}')\n",
    "    wandb.log({\"train_loss\": train_loss, \"valid_loss\": valid_loss,\n",
    "               \"train_auc\": train_auc, \"valid_auc\": valid_auc})\n",
    "    \n",
    "    if valid_loss < least_loss:\n",
    "        least_loss, num = valid_loss, 0\n",
    "    else:\n",
    "        num += 1\n",
    "        if num >= patience:\n",
    "            print(f'early stopped at {e} epoch')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7536527477769102, 0.7087290938911681)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(best_model)\n",
    "targets, preds, valid_loss, valid_auc = valid_step(model, loss_f, valid_dataloader, device)\n",
    "_, _, train_loss, train_auc = valid_step(model, loss_f, train_dataloader, device)\n",
    "train_auc, valid_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log({\n",
    "    \"final_train_auc\": train_auc,\n",
    "    \"final_valid_auc\": valid_auc,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[477, 221],\n",
       "       [170, 472]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(targets, np.array(preds) > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54354"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "# ROC 곡선 계산\n",
    "fpr, tpr, thresholds = roc_curve(targets, preds)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# 최적의 임계값 찾기\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "optimal_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1059222/2345342891.py:1: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  preds_ = [int(pred > optimal_threshold) for pred in preds]\n"
     ]
    }
   ],
   "source": [
    "preds_ = [int(pred > optimal_threshold) for pred in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a62c27910fe43a69bc40567c023b276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>final_train_auc</td><td>▁</td></tr><tr><td>final_valid_auc</td><td>▁</td></tr><tr><td>train_auc</td><td>▁▄▅▅▆▇▇▇████████▇▇▇▇▇▆</td></tr><tr><td>train_loss</td><td>█▆▅▅▄▃▂▂▁▁▁▁▁▁▁▁▂▂▂▂▃▃</td></tr><tr><td>valid_auc</td><td>▄█▂▁▅▇▅▅▅▅▄▄▄▄▅▂▄▄▄▄▄▆</td></tr><tr><td>valid_loss</td><td>▁▁▂▇▃▄▅▆▆▇▆▇▇▇▇▇▅▇▆█▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>final_train_auc</td><td>0.75365</td></tr><tr><td>final_valid_auc</td><td>0.70873</td></tr><tr><td>train_auc</td><td>0.78727</td></tr><tr><td>train_loss</td><td>0.44423</td></tr><tr><td>valid_auc</td><td>0.69615</td></tr><tr><td>valid_loss</td><td>0.67389</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lstm_240111-070506</strong> at: <a href='https://wandb.ai/dkt-recsys1/dkt-dl/runs/p3g6zvjq' target=\"_blank\">https://wandb.ai/dkt-recsys1/dkt-dl/runs/p3g6zvjq</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240111_070508-p3g6zvjq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.log({\n",
    "    \"confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "        probs=None,\n",
    "        y_true=targets, preds=preds_,\n",
    "        class_names=['0', '1'])})\n",
    "\n",
    "# finish wandb run\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test auc\n",
    "def test_step(model, loss_f, test_dataloader, device):\n",
    "    user_ids, test_pred_proba = [], []\n",
    "\n",
    "    for iter, data in enumerate(test_dataloader):\n",
    "        X = data['X'].float().to(device)\n",
    "        mask = data['mask'].float().to(device)\n",
    "        pred = model((X, mask))\n",
    "\n",
    "        user_ids.extend(data['user_id'].detach().numpy())\n",
    "        test_pred_proba.extend(torch.sigmoid(pred).detach().cpu().numpy())\n",
    "        \n",
    "    return user_ids, test_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids, test_pred_proba = test_step(model, loss_f, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.read_csv('../../data/sample_submission.csv')\n",
    "submission_df.prediction = np.array(test_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = config['result_dir']\n",
    "# now = dt.strftime(dt.now(), '%y%m%d-%H%M%S')\n",
    "modelname = config['modelname']\n",
    "savename = f'{modelname}_{now}_{valid_auc:.4f}.csv'\n",
    "submission_df.to_csv(os.path.join(result_dir, savename), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".dkt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
